apiVersion: v1
kind: Pod
metadata:
  namespace: cuda-ipc-dra
  name: cuda-ipc-producer-dra
  labels:
    app: cuda-ipc-producer-dra
spec:
  hostIPC: true
  hostPID: true
  containers:
  - name: producer
    image: nvidia/cuda:12.4.1-devel-ubuntu22.04
    command: ["/bin/bash", "-c"]
    args:
    - |
      echo "Producer: Listing available GPUs with nvidia-smi..."
      nvidia-smi -L
      echo "Producer: GPU UUIDs:"
      nvidia-smi --query-gpu=uuid --format=csv,noheader
      echo "Producer: GPU information complete."
      echo ""

      cat > /tmp/producer.cu << 'EOF'
      #include <cuda_runtime.h>
      #include <stdio.h>
      #include <unistd.h>
      #include <stdlib.h>

      int main() {
          void* devPtr;
          cudaIpcMemHandle_t handle;
          int deviceCount;

          printf("Producer: Initializing CUDA...\n");
          fflush(stdout);

          // Check available GPUs
          cudaError_t err = cudaGetDeviceCount(&deviceCount);
          if (err != cudaSuccess) {
              printf("ERROR getting device count: %s\n", cudaGetErrorString(err));
              return 1;
          }
          printf("Producer: Found %d GPU(s)\n", deviceCount);

          // List all available GPUs
          for (int i = 0; i < deviceCount; i++) {
              cudaDeviceProp prop;
              cudaGetDeviceProperties(&prop, i);
              printf("Producer: GPU %d: %s\n", i, prop.name);
          }
          fflush(stdout);

          err = cudaSetDevice(0);
          if (err != cudaSuccess) {
              printf("ERROR setting CUDA device: %s\n", cudaGetErrorString(err));
              return 1;
          }

          printf("Producer: Using GPU 0 for CUDA operations...\n");
          printf("Producer: Allocating GPU memory on device 0...\n");
          fflush(stdout);
          err = cudaMalloc(&devPtr, 1024 * 1024); // 1MB
          if (err != cudaSuccess) {
              printf("ERROR allocating GPU memory: %s\n", cudaGetErrorString(err));
              return 1;
          }

          printf("Producer: Writing test data to GPU memory...\n");
          fflush(stdout);
          int* hostData = (int*)malloc(1024 * 1024);
          for (int i = 0; i < 256 * 1024; i++) {
              hostData[i] = i + 42; // Simple pattern: index + 42
          }
          err = cudaMemcpy(devPtr, hostData, 1024 * 1024, cudaMemcpyHostToDevice);
          if (err != cudaSuccess) {
              printf("ERROR copying data to GPU: %s\n", cudaGetErrorString(err));
              return 1;
          }

          printf("Producer: Creating IPC handle...\n");
          fflush(stdout);
          err = cudaIpcGetMemHandle(&handle, devPtr);
          if (err != cudaSuccess) {
              printf("ERROR creating IPC handle: %s\n", cudaGetErrorString(err));
              return 1;
          }

          printf("Producer: Writing handle to shared volume...\n");
          fflush(stdout);
          FILE* f = fopen("/shared/cuda_ipc_handle.dat", "wb");
          if (!f) {
              printf("ERROR: Could not open handle file for writing\n");
              return 1;
          }
          fwrite(&handle, sizeof(handle), 1, f);
          fclose(f);

          printf("Producer: Success! Memory contains values 42, 43, 44, 45, 46...\n");
          printf("Producer: Hanging infinitely to keep GPU memory alive...\n");
          fflush(stdout);

          // Hang forever to keep the GPU memory allocated
          while (1) {
              sleep(3600);
          }

          return 0;
      }
      EOF

      echo "Compiling producer..."
      nvcc /tmp/producer.cu -o /tmp/producer
      echo "Starting producer..."
      /tmp/producer
    resources:
      claims:
      - name: shared-gpus
    securityContext:
      privileged: true
      capabilities:
        add: ["IPC_LOCK"]
    volumeMounts:
    - name: shared-volume
      mountPath: /shared

  resourceClaims:
  - name: shared-gpus
    resourceClaimName: shared-dual-gpus

  volumes:
  - name: shared-volume
    hostPath:
      path: /tmp/cuda-ipc-shared-dra
      type: DirectoryOrCreate

  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

  restartPolicy: Never